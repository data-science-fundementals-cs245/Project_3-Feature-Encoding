### 前期处理参数

####1. selective search

经过selective search 后，每张图片产生数十至二三百个proposal，处理时每张图片最多取100个proposal进行下一步的深度学习特征提取

#### 2. 深度学习特征提取

使用在ImageNet上预训练过的InceptionV3网络，固定前面所有卷积层的结构和权值的情况下，我自己添加了两层分别为1024维的全连接层和128维的全连接层，最后加上一个输出维度为50的用于分类的softmax的dense层。使用原始数据集（3万多张图像）进行第一次fine-tune，然后仅仅固定前249层网络，对后面的所有网络进行第二次fine-tune。 fine-tune准确率达到91.9%

深度学习特征选用上述网络的倒数第二层dense（全连接层）的输出（128维），输入为每个proposal（即从原始图像上截取下来的子图像）。

#### 3. feature encoding

该部分和sift的基本一致。在计算BOW和VLAD过程中，kmeans聚类的过程从每个类别中随机抽取了8000个proposal的深度学习特征（共计40万）当做kmeans聚类的数据集。

### 最终结果

#### DL + BOW

| k     | 16    | 32    | 64    | 128   |
| ----- | ----- | ----- | ----- | ----- |
| acc/% | 37.76 | 48.49 | 58.69 | 65.55 |

#### DL + VLAD

PCA 降维到1024维

| k    | 16    | 32    | 64    | 128   |
| ---- | ----- | ----- | ----- | ----- |
| PCA  | 81.17 | 78.03 | 74.75 | 68.09 |
| LDA  |       |       |       | 44.59 |

vlad变差原因，随着降维幅度的增加，损失的信息更多，因而结果变差

根据之前project1 的结论来看，如果选择如LDA这样的降维方法可能会取得比较好的效果:

**尝试了LDA降维，但是LDA无法超过50维，很影响准确率，只能达到40%+**

